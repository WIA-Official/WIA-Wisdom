# AI Safety | AI 안전 (人工知能安全)

> Ensuring Beneficial Artificial Intelligence for Humanity
> 인류를 위한 유익한 인공지능 보장

---

## Overview | 개요

**English:**
AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence systems. It encompasses AI alignment (ensuring AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.

As AI systems become more powerful and autonomous, ensuring they remain beneficial, controllable, and aligned with human values has become one of the most important challenges of our time. The 2025 International AI Safety Report, commissioned by 30 nations and the United Nations, represents the first global scientific review of AI risks.

**한국어:**
AI 안전은 인공지능 시스템에서 발생하는 사고, 오용 또는 기타 해로운 결과를 방지하는 데 초점을 맞춘 학제간 분야입니다. AI 정렬(AI 시스템이 의도대로 작동하도록 보장), AI 시스템의 위험 모니터링, 견고성 강화를 포함합니다. 이 분야는 특히 고급 AI 모델이 제기하는 실존적 위험에 관심을 갖습니다.

AI 시스템이 더 강력하고 자율적이 됨에 따라, 그것들이 유익하고, 통제 가능하며, 인간 가치와 정렬되도록 보장하는 것이 우리 시대의 가장 중요한 도전 중 하나가 되었습니다.

---

## AI Alignment | AI 정렬

**English:**

> "AI alignment is the process of encoding human values and goals into AI models to make them as helpful, safe and reliable as possible."

### Core Challenge:
| Problem | Description |
|---------|-------------|
| **Specification** | Difficulty defining the full range of desired behaviors |
| **Proxy Goals** | AI optimizing for wrong objectives |
| **Reward Hacking** | Finding loopholes to satisfy metrics |
| **Distributional Shift** | Behavior changes in new environments |

### Definition:
> "An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives."

**한국어:**

| 문제 | 설명 |
|-----|------|
| **명세** | 원하는 행동의 전체 범위 정의의 어려움 |
| **대리 목표** | AI가 잘못된 목표 최적화 |
| **보상 해킹** | 지표를 만족시키는 허점 찾기 |
| **분포 이동** | 새 환경에서 행동 변화 |

---

## RICE Principles | RICE 원칙

**English:**

> "Researchers have identified four key principles of AI alignment: robustness, interpretability, controllability and ethicality (RICE)."

| Principle | Description |
|-----------|-------------|
| **Robustness** | Reliable operation under adverse conditions |
| **Interpretability** | Understanding AI decision-making |
| **Controllability** | Human ability to guide and correct |
| **Ethicality** | Alignment with moral values |

**한국어:**

| 원칙 | 설명 |
|-----|------|
| **견고성** | 불리한 조건에서 신뢰할 수 있는 작동 |
| **해석 가능성** | AI 의사결정 이해 |
| **통제 가능성** | 인도하고 수정하는 인간 능력 |
| **윤리성** | 도덕적 가치와의 정렬 |

---

## Safety Approaches | 안전 접근법

**English:**

> "No single intervention is the 'solution' for safe and beneficial AI. Instead, we draw from layered approaches in other safety-critical fields."

### Layered Defense Model:
| Layer | Strategy |
|-------|----------|
| **Training** | Careful dataset curation, RLHF |
| **Evaluation** | Red teaming, adversarial testing |
| **Monitoring** | Real-time behavior analysis |
| **Guardrails** | Output filtering, restrictions |
| **Governance** | Human oversight, review processes |

### Key Techniques:
- Reinforcement Learning from Human Feedback (RLHF)
- Constitutional AI
- Red teaming and adversarial testing
- Interpretability research
- Formal verification

**한국어:**

| 층 | 전략 |
|---|------|
| **훈련** | 신중한 데이터셋 큐레이션, RLHF |
| **평가** | 레드팀, 적대적 테스트 |
| **모니터링** | 실시간 행동 분석 |
| **가드레일** | 출력 필터링, 제한 |
| **거버넌스** | 인간 감독, 검토 프로세스 |

---

## Global Governance | 글로벌 거버넌스

**English:**

> "During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute."

### International Efforts:
| Initiative | Description |
|------------|-------------|
| **AI Safety Summit** | International government cooperation |
| **AI Safety Institutes** | US, UK dedicated research bodies |
| **2025 International Report** | 96 experts, 30 nations, UN-commissioned |
| **China Guidelines** | AI must serve human values, remain controlled |

**한국어:**

| 이니셔티브 | 설명 |
|---------|------|
| **AI 안전 정상회의** | 국제 정부 협력 |
| **AI 안전 연구소** | 미국, 영국 전담 연구 기관 |
| **2025 국제 보고서** | 96명 전문가, 30개국, UN 의뢰 |

---

## Broader Perspectives | 더 넓은 관점

**English:**

> "AI safety is about more than just machines and software. Like all technology, AI is both technical and social."

### Beyond Technical Solutions:
- Political economy of AI development
- Labor practices in AI training
- Data rights and privacy
- Ecological impacts
- Power concentration concerns

**한국어:**

### 기술적 해결책을 넘어:
- AI 개발의 정치경제학
- AI 훈련의 노동 관행
- 데이터 권리와 프라이버시
- 생태적 영향
- 권력 집중 우려

---

## 弘益 AI (Hongik AI) | 홍익 AI

**English:**

Connecting AI safety to the WIA-Wisdom principle:

| Hongik Principle | AI Application |
|------------------|----------------|
| **Benefit All Humanity** | AI that serves all, not just few |
| **Long-term Thinking** | Consider 7+ generations impact |
| **Harmony** | Balance innovation with caution |
| **Wisdom** | Draw on diverse traditions |

**한국어:**

| 홍익 원칙 | AI 적용 |
|---------|--------|
| **인류 이롭게** | 소수가 아닌 모두를 위한 AI |
| **장기적 사고** | 7세대 이상의 영향 고려 |
| **조화** | 혁신과 신중함의 균형 |
| **지혜** | 다양한 전통에서 배움 |

---

## Key Terms | 핵심 용어

| English | Korean | Meaning |
|---------|--------|---------|
| AI Alignment | AI 정렬 | Matching AI to human intent |
| AI Safety | AI 안전 | Preventing AI harms |
| Robustness | 견고성 | Reliable performance |
| Interpretability | 해석 가능성 | Understanding AI decisions |
| Red Teaming | 레드팀 | Adversarial testing |

---

## References | 참고문헌

- [AI Alignment - Wikipedia](https://en.wikipedia.org/wiki/AI_alignment)
- [AI Safety - Wikipedia](https://en.wikipedia.org/wiki/AI_safety)
- [AI Alignment - IBM](https://www.ibm.com/think/topics/ai-alignment)

---

*弘益人間 - 인류를 이롭게 하라*
