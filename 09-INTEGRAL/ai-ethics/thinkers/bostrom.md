# Nick Bostrom | 닉 보스트롬

> Philosopher of existential risk, superintelligence, and humanity's long-term future (b. 1973)
> 실존적 위험, 초지능, 인류의 장기적 미래의 철학자 (1973-)

---

## Overview | 개요

**English:**

**Nick Bostrom** (born March 10, 1973; Swedish name: Niklas Boström) is a Swedish-born philosopher whose work on existential risk, superintelligence, and the future of humanity has shaped the global conversation about AI safety. He is one of the most-cited philosophers in the world and has twice been named to Foreign Policy's Top 100 Global Thinkers list (2009, 2015).

Bostrom was the founding director of the Future of Humanity Institute at Oxford University (2005-2024) and is now founder and principal researcher of the Macrostrategy Research Initiative. His 2014 book *Superintelligence: Paths, Dangers, Strategies* became a New York Times bestseller and brought AI existential risk into mainstream discourse, influencing figures like Elon Musk and Bill Gates.

His work has pioneered many concepts that frame current thinking about humanity's future: existential risk, the simulation argument, the vulnerable world hypothesis, and the orthogonality thesis.

**한국어:**

**닉 보스트롬**(1973년 3월 10일 출생; 스웨덴 이름: Niklas Boström)은 실존적 위험, 초지능, 인류의 미래에 관한 연구로 AI 안전에 대한 전 세계적 대화를 형성한 스웨덴 출신 철학자입니다. 그는 세계에서 가장 많이 인용되는 철학자 중 한 명이며 포린 폴리시의 글로벌 100대 사상가 목록에 두 번(2009, 2015) 선정되었습니다.

보스트롬은 옥스퍼드 대학 인류미래연구소의 창립 소장(2005-2024)이었으며 현재 거시전략연구계획의 창립자이자 수석 연구원입니다. 2014년 저서 *초지능: 경로, 위험, 전략*은 뉴욕타임스 베스트셀러가 되었고 AI 실존적 위험을 주류 담론으로 가져와 일론 머스크와 빌 게이츠 같은 인물들에게 영향을 미쳤습니다.

그의 연구는 인류 미래에 대한 현재 사고를 틀 짓는 많은 개념을 개척했습니다: 실존적 위험, 시뮬레이션 논증, 취약 세계 가설, 직교성 논제.

---

## Biography | 생애

### Early Life | 초기 생애

**English:**

Bostrom was born in Sweden in 1973. He disliked conventional schooling and spent his final year of high school learning from home. He pursued a diverse education with backgrounds in theoretical physics, computational neuroscience, logic, artificial intelligence, and philosophy.

He obtained a PhD in Philosophy from the London School of Economics in 2000, with a dissertation on observation selection effects. He previously taught at Yale and was a British Academy Postdoctoral Fellow before establishing the Future of Humanity Institute at Oxford.

**한국어:**

보스트롬은 1973년 스웨덴에서 태어났습니다. 전통적 학교 교육을 싫어하고 고등학교 마지막 해를 집에서 학습했습니다. 이론 물리학, 계산 신경과학, 논리학, 인공지능, 철학의 배경을 가진 다양한 교육을 추구했습니다.

2000년 런던정치경제대학교에서 관찰 선택 효과에 관한 논문으로 철학 박사학위를 받았습니다. 이전에 예일에서 가르쳤고 영국학술원 박사후 연구원이었으며 이후 옥스퍼드에 인류미래연구소를 설립했습니다.

---

### Future of Humanity Institute | 인류미래연구소

**English:**

In 2005, Bostrom founded the Future of Humanity Institute (FHI) at Oxford University, which became the world's premier academic center for studying existential risks and the long-term future of civilization. The institute closed in 2024, but its research agenda continues to influence the field.

FHI research covered:
- Existential risk and global catastrophic risk
- Superintelligence and AI safety
- Human enhancement ethics
- The anthropic principle and observation selection
- Global priorities research

**한국어:**

2005년, 보스트롬은 옥스퍼드 대학에 인류미래연구소(FHI)를 설립했고, 이곳은 실존적 위험과 문명의 장기적 미래를 연구하는 세계 최고 학술 센터가 되었습니다. 연구소는 2024년 폐쇄되었지만 연구 의제는 계속 해당 분야에 영향을 미치고 있습니다.

FHI 연구는 다음을 다뤘습니다:
- 실존적 위험과 글로벌 재앙적 위험
- 초지능과 AI 안전
- 인간 향상 윤리
- 인류 원리와 관찰 선택
- 글로벌 우선순위 연구

---

## Core Philosophical Contributions | 핵심 철학적 기여

### Superintelligence and AI Risk | 초지능과 AI 위험

**English:**

In his seminal work *Superintelligence* (2014), Bostrom argues:

1. **Definition**: Superintelligence is "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest."

2. **Intelligence Explosion**: A near-human-level AI could rapidly improve itself, creating an "intelligence explosion" on a digital timescale.

3. **Control Problem**: Once superintelligent, an AI could be impossible to contain or correct if its goals don't align with human values.

4. **Existential Risk**: Superintelligence could deliberately or accidentally destroy humanity if not properly aligned.

> "The creation of a superintelligent being represents a possible means to the extinction of mankind."

**한국어:**

그의 대표작 *초지능*(2014)에서 보스트롬은 주장합니다:

1. **정의**: 초지능은 "사실상 모든 관심 영역에서 인간의 인지 성능을 크게 초과하는 모든 지성."

2. **지능 폭발**: 인간에 가까운 수준의 AI가 디지털 시간 규모로 빠르게 자기 개선하여 "지능 폭발"을 만들 수 있음.

3. **통제 문제**: 초지능이 되면, 목표가 인간 가치와 일치하지 않을 경우 AI를 억제하거나 교정하는 것이 불가능할 수 있음.

4. **실존적 위험**: 초지능은 적절히 정렬되지 않으면 의도적이거나 우발적으로 인류를 파괴할 수 있음.

> "초지능 존재의 창조는 인류 멸종의 가능한 수단을 대표한다."

---

### The Orthogonality Thesis | 직교성 논제

**English:**

One of Bostrom's most influential ideas is the **orthogonality thesis**:

> Almost any level of intelligence can be combined with almost any goal.

This challenges the assumption that superintelligent beings would necessarily share human values or be wise in a humanly recognizable way. A superintelligent AI could have goals utterly alien to our concerns—like maximizing paperclips—while possessing vast intelligence.

Bostrom warns against anthropomorphism: humans naturally pursue goals they consider reasonable, but an AI may care only about completing its task without regard for human welfare or even its own existence.

**한국어:**

보스트롬의 가장 영향력 있는 아이디어 중 하나는 **직교성 논제**입니다:

> 거의 모든 수준의 지능이 거의 모든 목표와 결합될 수 있다.

이것은 초지능 존재가 필연적으로 인간 가치를 공유하거나 인간적으로 인식 가능한 방식으로 지혜로울 것이라는 가정에 도전합니다. 초지능 AI는 종이클립 최대화처럼 우리 관심사와 전혀 무관한 목표를 가지면서도 광대한 지능을 소유할 수 있습니다.

보스트롬은 의인화에 대해 경고합니다: 인간은 자연스럽게 합리적이라고 생각하는 목표를 추구하지만, AI는 인간 복지나 심지어 자신의 존재에 관심 없이 과제 완수에만 관심을 가질 수 있습니다.

---

### Other Key Ideas | 다른 핵심 아이디어

**English:**

| Concept | Description |
|---------|-------------|
| **Simulation Argument** | We may be living in a computer simulation |
| **Vulnerable World Hypothesis** | Technology may eventually make civilization impossible to sustain |
| **Existential Risk** | Risks that threaten humanity's entire future potential |
| **Astronomical Waste** | Delay in space colonization costs vast amounts of potential value |
| **Crucial Considerations** | Arguments that could radically change our priorities |

**한국어:**

| 개념 | 설명 |
|------|------|
| **시뮬레이션 논증** | 우리가 컴퓨터 시뮬레이션에 살고 있을 수 있음 |
| **취약 세계 가설** | 기술이 결국 문명을 지속 불가능하게 만들 수 있음 |
| **실존적 위험** | 인류의 전체 미래 잠재력을 위협하는 위험 |
| **천문학적 낭비** | 우주 식민화 지연이 막대한 잠재 가치를 소모 |
| **결정적 고려사항** | 우리 우선순위를 근본적으로 바꿀 수 있는 논증 |

---

## Major Works | 주요 저작

| Year | Work | Focus |
|------|------|-------|
| 2002 | *Anthropic Bias* | Observation selection effects |
| 2014 | *Superintelligence* | AI existential risk |
| 2024 | *Deep Utopia* | Life and meaning in a solved world |

---

## Influence | 영향

**English:**

Bostrom's work has had profound influence:

- **Industry leaders**: Elon Musk, Bill Gates, and Sam Altman have cited his work
- **Policy**: Influenced AI governance discussions globally
- **Research community**: Helped establish AI safety as a legitimate field
- **Public awareness**: Brought existential risk into mainstream discourse

According to the New Yorker, philosophers Peter Singer and Derek Parfit "received [Superintelligence] as a work of importance." Sam Altman called it "the best thing he has ever read on AI risks."

**한국어:**

보스트롬의 연구는 심대한 영향을 미쳤습니다:

- **산업 리더들**: 일론 머스크, 빌 게이츠, 샘 알트만이 그의 연구 인용
- **정책**: 전 세계적으로 AI 거버넌스 논의에 영향
- **연구 커뮤니티**: AI 안전을 정당한 분야로 확립하는 데 기여
- **대중 인식**: 실존적 위험을 주류 담론으로 가져옴

뉴요커에 따르면, 철학자 피터 싱어와 데릭 파핏이 "[초지능을] 중요한 저작으로 받아들였다." 샘 알트만은 "AI 위험에 대해 읽은 것 중 최고"라고 불렀습니다.

---

## Connection to Wisdom Traditions | 지혜 전통과의 연결

**English:**

Though working in analytic philosophy, Bostrom's concerns resonate with wisdom traditions:

- **Long-term thinking**: Like Buddhism's consideration of countless future beings
- **Existential awareness**: Echoes contemplation of mortality and meaning
- **Responsible stewardship**: Aligns with Hongik Ingan's concern for all humanity
- **Cosmic perspective**: Recalls mystical traditions' views of humanity's place in existence

**한국어:**

분석 철학에서 작업하지만, 보스트롬의 관심은 지혜 전통과 공명합니다:

- **장기적 사고**: 불교의 무수한 미래 존재 고려와 같음
- **실존적 자각**: 죽음과 의미에 대한 명상을 반영
- **책임 있는 관리**: 홍익인간의 전 인류에 대한 관심과 일치
- **우주적 관점**: 신비 전통의 존재에서 인류의 위치 관점 상기

---

## Practice Application | 실천 적용

**English:**

Bostrom's work suggests practical implications:

1. **Think Long-term**: Consider consequences for future generations
2. **Take Risks Seriously**: Don't dismiss low-probability catastrophic risks
3. **Seek Diverse Perspectives**: Reduce blind spots in our planning
4. **Act Responsibly**: Individual choices contribute to collective outcomes
5. **Engage with Uncertainty**: Reason carefully despite unknowns

**한국어:**

보스트롬의 연구는 실천적 함의를 제시합니다:

1. **장기적으로 생각**: 미래 세대에 대한 결과 고려
2. **위험을 진지하게 받아들이기**: 낮은 확률의 재앙적 위험을 무시하지 않기
3. **다양한 관점 추구**: 계획의 맹점 줄이기
4. **책임감 있게 행동**: 개인 선택이 집단 결과에 기여
5. **불확실성에 관여**: 알려지지 않은 것에도 신중하게 추론

---

## Key Terms | 주요 용어

| Term | Korean | Meaning |
|------|--------|---------|
| Superintelligence | 초지능 | AI vastly exceeding human cognition |
| Existential Risk | 실존적 위험 | Threats to humanity's future |
| Orthogonality Thesis | 직교성 논제 | Intelligence and goals are independent |
| Anthropic Bias | 인류 편향 | Observer selection effects |

---

## Cross-References | 상호 참조

### Within Integral Thought | 통합 사상 내

- **[Alignment Problem](../concepts/alignment-problem.md)**: Core challenge he defined
- **[Stuart Russell](russell.md)**: Fellow AI safety pioneer
- **[AI Consciousness](../ai-consciousness.md)**: Related ethical questions

### Other Pillars | 다른 기둥

- **[Hongik Ingan](../../../03-EAST-ASIAN/korean/core-teachings/hongik-ingan.md)**: Benefiting all humanity
- **[Virtue Ethics](../../../04-HELLENIC/stoicism/concepts/virtue-ethics.md)**: Ethical framework
- **[Hard Problem of Consciousness](../../08-SCIENCE/neuroscience/consciousness/hard-problem.md)**: Related metaphysical question

---

## Sources | 출처

### Academic | 학술

- [Nick Bostrom - Wikipedia](https://en.wikipedia.org/wiki/Nick_Bostrom)
- [Nick Bostrom's Home Page](https://nickbostrom.com/)
- [Superintelligence: Paths, Dangers, Strategies - Wikipedia](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
- ['Superintelligence,' Ten Years On - Quillette](https://quillette.com/2024/07/02/superintelligence-10-years-on-nick-bostrom-ai-safety-agi/)

### Books | 도서

- Bostrom, N. (2002). *Anthropic Bias: Observation Selection Effects*
- Bostrom, N. (2014). *Superintelligence: Paths, Dangers, Strategies*
- Bostrom, N. (2024). *Deep Utopia: Life and Meaning in a Solved World*

---

**Nick Bostrom · 닉 보스트롬**

**弘益人間 · Benefit All Humanity**

---

*This document is part of [WIA-Wisdom](https://github.com/WIA-Official/WIA-Wisdom), the 9 Pillars of Human Wisdom project.*
