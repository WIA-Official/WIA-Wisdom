# Nick Bostrom | 닉 보스트롬

> Philosopher of existential risk, superintelligence, and humanity's long-term future (b. 1973)

---

## Overview | 개요

**Nick Bostrom** (born March 10, 1973; Swedish name: Niklas Boström) is a Swedish-born philosopher whose work on existential risk, superintelligence, and the future of humanity has shaped the global conversation about AI safety. He is one of the most-cited philosophers in the world and has twice been named to Foreign Policy's Top 100 Global Thinkers list (2009, 2015).

Bostrom was the founding director of the Future of Humanity Institute at Oxford University (2005-2024) and is now founder and principal researcher of the Macrostrategy Research Initiative. His 2014 book *Superintelligence: Paths, Dangers, Strategies* became a New York Times bestseller and brought AI existential risk into mainstream discourse, influencing figures like Elon Musk and Bill Gates.

His work has pioneered many concepts that frame current thinking about humanity's future: existential risk, the simulation argument, the vulnerable world hypothesis, and the orthogonality thesis.

---
## Biography | 생애
### Early Life | 초기 생애

Bostrom was born in Sweden in 1973. He disliked conventional schooling and spent his final year of high school learning from home. He pursued a diverse education with backgrounds in theoretical physics, computational neuroscience, logic, artificial intelligence, and philosophy.

He obtained a PhD in Philosophy from the London School of Economics in 2000, with a dissertation on observation selection effects. He previously taught at Yale and was a British Academy Postdoctoral Fellow before establishing the Future of Humanity Institute at Oxford.

---
### Future of Humanity Institute | 인류미래연구소

In 2005, Bostrom founded the Future of Humanity Institute (FHI) at Oxford University, which became the world's premier academic center for studying existential risks and the long-term future of civilization. The institute closed in 2024, but its research agenda continues to influence the field.

FHI research covered:
- Existential risk and global catastrophic risk
- Superintelligence and AI safety
- Human enhancement ethics
- The anthropic principle and observation selection
- Global priorities research

---
## Core Philosophical Contributions | 핵심 철학적 기여
### Superintelligence and AI Risk | 초지능과 AI 위험

In his seminal work *Superintelligence* (2014), Bostrom argues:

1. **Definition**: Superintelligence is "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest."

2. **Intelligence Explosion**: A near-human-level AI could rapidly improve itself, creating an "intelligence explosion" on a digital timescale.

3. **Control Problem**: Once superintelligent, an AI could be impossible to contain or correct if its goals don't align with human values.

4. **Existential Risk**: Superintelligence could deliberately or accidentally destroy humanity if not properly aligned.

> "The creation of a superintelligent being represents a possible means to the extinction of mankind."

> "초지능 존재의 창조는 인류 멸종의 가능한 수단을 대표한다."
---
### The Orthogonality Thesis | 직교성 논제

One of Bostrom's most influential ideas is the **orthogonality thesis**:

> Almost any level of intelligence can be combined with almost any goal.

This challenges the assumption that superintelligent beings would necessarily share human values or be wise in a humanly recognizable way. A superintelligent AI could have goals utterly alien to our concerns—like maximizing paperclips—while possessing vast intelligence.

Bostrom warns against anthropomorphism: humans naturally pursue goals they consider reasonable, but an AI may care only about completing its task without regard for human welfare or even its own existence.

> 거의 모든 수준의 지능이 거의 모든 목표와 결합될 수 있다.
---
### Other Key Ideas | 다른 핵심 아이디어

| Concept | Description |
|---------|-------------|
| **Simulation Argument** | We may be living in a computer simulation |
| **Vulnerable World Hypothesis** | Technology may eventually make civilization impossible to sustain |
| **Existential Risk** | Risks that threaten humanity's entire future potential |
| **Astronomical Waste** | Delay in space colonization costs vast amounts of potential value |
| **Crucial Considerations** | Arguments that could radically change our priorities |

| 개념 | 설명 |
|------|------|
| **시뮬레이션 논증** | 우리가 컴퓨터 시뮬레이션에 살고 있을 수 있음 |
| **취약 세계 가설** | 기술이 결국 문명을 지속 불가능하게 만들 수 있음 |
| **실존적 위험** | 인류의 전체 미래 잠재력을 위협하는 위험 |
| **천문학적 낭비** | 우주 식민화 지연이 막대한 잠재 가치를 소모 |
| **결정적 고려사항** | 우리 우선순위를 근본적으로 바꿀 수 있는 논증 |
---
## Major Works | 주요 저작
| Year | Work | Focus |
|------|------|-------|
| 2002 | *Anthropic Bias* | Observation selection effects |
| 2014 | *Superintelligence* | AI existential risk |
| 2024 | *Deep Utopia* | Life and meaning in a solved world |
---
## Influence | 영향

Bostrom's work has had profound influence:

- **Industry leaders**: Elon Musk, Bill Gates, and Sam Altman have cited his work
- **Policy**: Influenced AI governance discussions globally
- **Research community**: Helped establish AI safety as a legitimate field
- **Public awareness**: Brought existential risk into mainstream discourse

According to the New Yorker, philosophers Peter Singer and Derek Parfit "received [Superintelligence] as a work of importance." Sam Altman called it "the best thing he has ever read on AI risks."

---
## Connection to Wisdom Traditions | 지혜 전통과의 연결

Though working in analytic philosophy, Bostrom's concerns resonate with wisdom traditions:

- **Long-term thinking**: Like Buddhism's consideration of countless future beings
- **Existential awareness**: Echoes contemplation of mortality and meaning
- **Responsible stewardship**: Aligns with Hongik Ingan's concern for all humanity
- **Cosmic perspective**: Recalls mystical traditions' views of humanity's place in existence

---
## Practice Application | 실천 적용

Bostrom's work suggests practical implications:

1. **Think Long-term**: Consider consequences for future generations
2. **Take Risks Seriously**: Don't dismiss low-probability catastrophic risks
3. **Seek Diverse Perspectives**: Reduce blind spots in our planning
4. **Act Responsibly**: Individual choices contribute to collective outcomes
5. **Engage with Uncertainty**: Reason carefully despite unknowns

---
## Key Terms | 주요 용어
| Term | Korean | Meaning |
|------|--------|---------|
| Superintelligence | 초지능 | AI vastly exceeding human cognition |
| Existential Risk | 실존적 위험 | Threats to humanity's future |
| Orthogonality Thesis | 직교성 논제 | Intelligence and goals are independent |
| Anthropic Bias | 인류 편향 | Observer selection effects |
---
## Cross-References | 상호 참조

### Within Integral Thought | 통합 사상 내

- **[Alignment Problem](../concepts/alignment-problem.md)**: Core challenge he defined
- **[Stuart Russell](russell.md)**: Fellow AI safety pioneer
### Other Pillars | 다른 기둥
---
## Sources | 출처

### Academic | 학술

- [Nick Bostrom - Wikipedia](https://en.wikipedia.org/wiki/Nick_Bostrom)
- [Nick Bostrom's Home Page](https://nickbostrom.com/)
### Books | 도서
---
**弘益人間 · Benefit All Humanity**
---
*This document is part of [WIA-Wisdom](https://github.com/WIA-Official/WIA-Wisdom), the 9 Pillars of Human Wisdom project.*