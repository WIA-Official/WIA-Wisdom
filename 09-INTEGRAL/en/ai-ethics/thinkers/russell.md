# Stuart Russell | 스튜어트 러셀

> Pioneer of human-compatible AI and the value alignment approach (b. 1962)

---

## Overview | 개요

**Stuart Russell** is a British-American computer scientist and philosopher who has become one of the world's leading voices on AI safety. He is Professor of Computer Science at the University of California, Berkeley, and co-author (with Peter Norvig) of *Artificial Intelligence: A Modern Approach*—the standard textbook used by universities worldwide, sometimes called "the AIMA book."

Russell's 2019 book *Human Compatible: Artificial Intelligence and the Problem of Control* redefines what AI should be, proposing that AI systems should be designed to be uncertain about human preferences and deferential to human judgment. His work represents a fundamental shift from "building intelligent machines" to "building machines that are provably beneficial to humans."

He founded and directs the Center for Human-Compatible AI (CHAI) at UC Berkeley, leading research on how to create AI systems that remain safe and beneficial as they become more powerful.

---
## Biography | 생애

Stuart Russell was born in 1962 in Portsmouth, England. He received his B.A. with first-class honours in Physics from Wadham College, Oxford (1982) and his Ph.D. in Computer Science from Stanford University (1986).

Since joining UC Berkeley in 1986, he has become one of the most influential figures in AI. His textbook with Peter Norvig (first edition 1995) has been translated into more than 25 languages and is used by 1,500+ universities worldwide.

Russell has received numerous awards, including the IJCAI Computers and Thought Award (1995), fellowship in the American Association for Artificial Intelligence, and the Association for Computing Machinery.

---
## Core Contributions | 핵심 기여
### Rethinking AI's Foundation | AI 기반 재고

Russell argues that the "standard model" of AI—building machines that optimize given objectives—is fundamentally dangerous. The problem isn't with AI being evil, but with AI being effective at pursuing objectives that may not perfectly capture what humans actually want.

> "We may, perhaps inadvertently, imbue machines with objectives that are not well aligned with human values. This problem requires a change in the definition of AI itself."

His solution: redefine AI from a field concerned with *pure intelligence* to one concerned with *systems that are provably beneficial to humans*.

> "우리는 아마도 무심코 인간 가치와 잘 맞지 않는 목표를 기계에 부여할 수 있다. 이 문제는 AI 자체의 정의 변경을 요구한다."
---
### Three Principles for Beneficial AI | 유익한 AI를 위한 세 원칙

In *Human Compatible*, Russell proposes three principles for designing AI that doesn't conflict with human purposes:

**Principle 1: Altruism**
> "The machine's only objective is to maximize the realization of human preferences."

The AI has no purpose of its own and no innate desire to protect itself.

**Principle 2: Humility**
> "The machine is initially uncertain about what those preferences are."

This uncertainty is crucial—it prevents the AI from acting dogmatically on a potentially incorrect model of human values.

**Principle 3: Learning from Humans**
> "The ultimate source of information about human preferences is human behavior."

The AI learns by observing and interacting with humans, continuously refining its understanding.

> "기계의 유일한 목표는 인간 선호의 실현을 최대화하는 것."
> "기계는 그 선호가 무엇인지 처음에 불확실함."
> "인간 선호에 대한 정보의 궁극적 출처는 인간 행동."
---
### Inverse Reinforcement Learning | 역강화학습

Russell pioneered **inverse reinforcement learning (IRL)**—a technique where AI systems learn human preferences by observing human behavior rather than being given explicit reward functions.

Instead of programming "maximize X," IRL asks: "What objective would explain the behavior I'm observing?" This approach:
- Avoids misspecification of goals
- Keeps the AI deferential to human correction
- Aligns AI learning with the complexity of human values

---
## The Value Alignment Problem | 가치 정렬 문제

Russell coined the term "value alignment problem" to describe the challenge:

> "AI systems will operate with increasing autonomy and capability in complex domains in the real world. The question is: how can we ensure that they have the right behavioral dispositions—the goals or 'values' needed to ensure that things turn out well, from a human point of view?"

Key insights:
- The alignment problem exists even for relatively unintelligent AI systems
- It's not about preventing evil AI but about precise specification of values
- There is cause for optimism if we recognize alignment as an intrinsic part of AI research

> "AI 시스템은 현실 세계의 복잡한 영역에서 점점 더 많은 자율성과 능력으로 작동할 것이다. 질문은: 인간 관점에서 일이 잘 되도록 보장하는 데 필요한 올바른 행동 성향—목표 또는 '가치'—을 갖추도록 어떻게 보장할 수 있는가?"
---
## Center for Human-Compatible AI (CHAI) | 인간호환AI센터

Russell founded CHAI at UC Berkeley to develop new theoretical and empirical approaches to AI alignment. Research areas include:

- **Assistance games**: Formal models where AI's objective is helping humans achieve their preferences
- **Preference learning**: Methods for inferring human values from behavior
- **Off-switch problem**: Ensuring AI allows itself to be corrected or turned off
- **Safe interruptibility**: AI systems that don't resist shutdown
- **Scalable oversight**: Human supervision of increasingly capable AI

---
## Key Works | 주요 저작
| Year | Work | Description |
|------|------|-------------|
| 1995 | *Artificial Intelligence: A Modern Approach* (with Norvig) | Standard AI textbook |
| 2019 | *Human Compatible* | Redefining AI for safety |
| Various | CHAI Research Papers | Technical alignment research |
---
## Connection to Wisdom Traditions | 지혜 전통과의 연결

Russell's approach resonates with ancient wisdom:

- **Humility**: The AI remaining uncertain echoes philosophical traditions valuing epistemic humility
- **Service**: The AI serving human flourishing recalls concepts like Hongik Ingan (benefiting humanity)
- **Learning**: Continuous learning from human behavior parallels master-student relationships
- **Deference**: AI deferring to human judgment reflects values of respect and non-domination

---
## Practice Application | 실천 적용

Russell's principles for AI developers and organizations:

1. **Build Uncertainty In**: Don't assume you know what users want—design systems to learn
2. **Allow Correction**: Make systems that can be safely interrupted and modified
3. **Observe Before Acting**: Gather information about preferences before taking irreversible actions
4. **Think Beneficially**: Shift from "does it work?" to "is it good for people?"
5. **Collaborate**: AI alignment requires multidisciplinary cooperation

---
## Key Terms | 주요 용어
| Term | Korean | Meaning |
|------|--------|---------|
| Human-Compatible | 인간 호환 | AI designed to benefit humans |
| Value Alignment | 가치 정렬 | Matching AI goals to human values |
| Inverse RL | 역강화학습 | Learning objectives from behavior |
| Assistance Game | 보조 게임 | Formal model of helpful AI |
---
## Cross-References | 상호 참조

### Within Integral Thought | 통합 사상 내

- **[Alignment Problem](../concepts/alignment-problem.md)**: Core challenge he addresses
- **[Nick Bostrom](bostrom.md)**: Fellow AI safety pioneer
### Other Pillars | 다른 기둥
---
## Sources | 출처

### Academic | 학술

- [Human Compatible - Wikipedia](https://en.wikipedia.org/wiki/Human_Compatible)
- [Stuart Russell: AI value alignment problem - LessWrong](https://www.lesswrong.com/posts/S95qCHBXtASmYyGSs/stuart-russell-ai-value-alignment-problem-must-be-an)
### Books | 도서
---
**弘益人間 · Benefit All Humanity**
---
*This document is part of [WIA-Wisdom](https://github.com/WIA-Official/WIA-Wisdom), the 9 Pillars of Human Wisdom project.*